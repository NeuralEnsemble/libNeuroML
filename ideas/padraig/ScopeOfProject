Some ideas on the scope of the project 


* NeuroML

NeuroML will provide the core object model for describing neuronal morphologies, ion channels, synapses and 
3D network structure of the API.
Any dynamical components (channels, synapses, abstract cell models) will have a definition "behind the scenes" in LEMS.
However, all NeuroML will specify is that "element segment will contain element distal with attributes x, y, z, 
diameter..." or "element izhikevichCell will have attributes a, b, c...".


* Serialisations

A) The XML serialisation will be the "natural" serialisation and will be a pretty faithful reflection of the NeuroML
object model. The format of the XML will be specified by the XML Schema definition (XSD file). Note: LEMS definitions 
and this XSD file are manually kept in line. 
B) There should be a standardised (and verifiable) HDF5 serialisation. This can be based on the overall structure of the 
NeuroML object model, but use optimised internal representations of 1) neuronal morphologies, 2) cell locations and 3) 
synaptic connectivity. This can be based on Stephan's examples for representing morphs & networks in NeuroHDF.
C) Other serialisation formats could be envisioned, e.g. YAML. That format would most likely have a one to one mapping 
to the XML format. Other binary formats (e.g. Binary XML) may be unnecessary, since HDF5 should cover most of our needs.


* APIs for accessing NeuroML files (libNeuroML instances)

APIs in any given language (e.g. Java, Python) should be independent of the serialisation format, and be based on the 
NeuroML object model.
Core read/write methods will be here, e.g. read_neuroml(xmlOrHdfFormat), get_cells(), cell.get_segments(), get_populations(), 
get_projections(), cell.add_segment(), cell.getsegments_in_group(grp), add_population(), save_as_xml(), save_as_hdf5(), 
etc. Additional helper methods could be envisioned across the implementations, e.g. cell.retrieveCoordinates(), etc. 

The naming convention of methods will be a thorny issue: I'd argue that it should be uniform across 
language implementations, as it is in libSBML (camelCase, e.g. cell.getSegments()), but the concensus for now seems 
to be to use PEP8 for the Python API, encouraging a more Pythonic API, e.g. cell.get_segments() as well as cell.segments


* Backends
 
A backend consists of a specific implementation which stores structure of the cells, network, etc. when the
model is loaded in from XML/HDF5. Scenarios for this can include:

A) A language specific object model generated from the NeuroML schema, e.g. Java or Python class hierarchy for Cell, 
Segment, Network, etc. is used to store the data
B) Optimised version of above with certain objects substituted for more optimal representations of data (e.g. a numpy 
array for 3D points) 
C) Simulator specific backend, i.e. when add_cell(...) or cell.add_segment() are called, equivalent entities 
in the native simulator are created. Similarly when save_as_xml() is called in the API the native object tree is parsed for 
this information.

Scenario A) above already has an initial implementation in Python (see 
https://github.com/NeuralEnsemble/libNeuroML/tree/master/ideas/padraig/generatedFromV2Schema).
Scenario B) would fulfil most people's requirements for a standalone Python API to read/write large data sets of 
cell/network data, and could be build from A
Scenario C) could be implemented in Python by extending PyNN for multicompartmental simulators, and by hooking 
commands for building networks in libNeuroML (e.g. addCell(), addPopulation, createSegment()) to existing or new 
PyNN commands, and similarly parsing the simulator's internally instantiated network with PyNN commands when the 
network needs to be saved to XML/HDF5. This would facilitate running/saving/plotting models on PyNN simulators.
A version of Mike Vella's example file https://github.com/NeuralEnsemble/libNeuroML/blob/master/ideas/mike/example.py
would certainly be possible through a libNeuroML+PyNN API like this.

(Padraig: I plan to improve the existing implementation of A, try out some ideas for substituting numpy arrays in this
to try B, and play with pyNeuron for testing ideas towards C, and would be happy to help extend PyNN for this)

* Extensions

The core libNeuroML API should be kept to the minimal set of commands for reading/writing/parsing the object tree.
Extension packages can be built on this (and PyNN) which use this functionality with specific workflows in mind, e.g.
building small networks of multicomp neurons with MorphForge, parameter fitting neuron models (Mike V's package).

Other packages then can be developed (e.g. for visualisation in 3D, for analysis of morphological or network properties) which will be 
usable on any simulator which supports libNeuroML+PyNN, e.g. NEURON, MOOSE...

